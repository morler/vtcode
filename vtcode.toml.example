# VTCode Configuration File (Example)
# This is a minimal configuration example for getting started with vtcode.
# Copy this file to vtcode.toml and customize as needed.

[agent]
# Basic agent settings
max_conversation_turns = 1000
max_session_duration_minutes = 60
verbose_logging = false

# AI Provider configuration (supports "gemini", "openai", "anthropic", "openrouter", "xai")
provider = "gemini"
default_model = "gemini-2.5-flash"
api_key_env = "GEMINI_API_KEY"
default_system_instruction = "You are a helpful coding assistant."
refine_prompts_enabled = false
refine_prompts_max_passes = 1
refine_prompts_model = ""
enable_self_review = false
max_review_passes = 1

# UI theme applied to ANSI output (options: "ciapre-dark", "ciapre-blue")
theme = "ciapre-dark"
# Chat UI surface: "auto" (default), "alternate" (always alternate screen), or "inline"
ui_surface = "auto"

[agent.onboarding]
enabled = true
intro_text = "Welcome! I preloaded workspace context so you can focus on decisions."
include_project_overview = true
include_language_summary = true
include_guideline_highlights = true
guideline_highlight_limit = 3
usage_tips = [
    "Share your goal or ask for /status to recap configuration.",
    "Reference AGENTS.md norms before editing.",
    "Draft or refresh your TODOs with update_plan before modifying code.",
    "Prefer targeted reads (read_file, grep_search) before modifying code.",
]
recommended_actions = [
    "Outline a 3-6 step TODO plan using update_plan before diving in.",
    "Request a quick workspace orientation.",
    "Describe the change you want to pursue next.",
]
chat_placeholder = "Sketch the TODO plan you want me to track (use update_plan)."

[tools]
# Default policy for tools: "allow", "prompt", or "deny"
default_policy = "prompt"

# Maximum inner tool-call loops per user turn
# Prevents infinite tool-calling cycles during chat
max_tool_loops = 100

# Override default policy for specific tools
[tools.policies]
read_file = "allow"
write_file = "prompt"
delete_file = "deny"
run_terminal_cmd = "prompt"

[commands]
# Safe commands that execute without confirmation
allow_list = [
    "ls", "pwd", "cat", "grep", "git status", "cargo check"
]

# Dangerous commands that are always blocked
deny_list = [
    "rm -rf", "sudo rm", "shutdown", "format"
]

# Commands requiring extra confirmation
dangerous_patterns = [
    "rm -f", "git reset --hard", "pip install"
]

[security]
human_in_the_loop = true
confirm_destructive_actions = true
log_all_commands = true
max_file_size_mb = 50
# Common programming language extensions
allowed_file_extensions = [
    ".rs", ".toml", ".json", ".md", ".txt", ".yaml", ".yml",
    ".js", ".ts", ".py", ".go", ".java", ".cpp", ".c", ".h"
]

[pty]
enabled = true
default_rows = 24
default_cols = 80
max_sessions = 10
command_timeout_seconds = 300

[router]
enabled = true
heuristic_classification = true
llm_router_model = ""

[router.models]
simple = "gemini-2.5-flash-lite"
standard = "gemini-2.5-flash"
complex = "gemini-2.5-pro"
codegen_heavy = "gemini-2.5-flash"
retrieval_heavy = "gemini-2.5-flash"

[telemetry]
trajectory_enabled = true

[prompt_cache]
enabled = true
cache_dir = ".vtcode/cache/prompts"
max_entries = 1000
max_age_days = 30
enable_auto_cleanup = true
min_quality_threshold = 0.7

[prompt_cache.providers.openai]
enabled = true
min_prefix_tokens = 1024
idle_expiration_seconds = 3600
surface_metrics = true

[prompt_cache.providers.anthropic]
enabled = true
default_ttl_seconds = 300
extended_ttl_seconds = 3600
max_breakpoints = 4
cache_system_messages = true
cache_user_messages = true

[prompt_cache.providers.gemini]
enabled = true
mode = "implicit"
min_prefix_tokens = 1024
explicit_ttl_seconds = 3600

[prompt_cache.providers.openrouter]
enabled = true
propagate_provider_capabilities = true
report_savings = true

[prompt_cache.providers.xai]
enabled = true

[prompt_cache.providers.deepseek]
enabled = true
surface_metrics = true

[context.ledger]
enabled = true
max_entries = 12
include_in_prompt = true
preserve_in_compression = true

[context.token_budget]
# Enable token budget tracking for attention management
enabled = true
# Model for tokenizer - examples from latest models:
# - GPT: "gpt-5", "gpt-5-mini", "gpt-4o-mini"
# - Claude: "claude-sonnet-4", "claude-opus-4", "claude-3.5-haiku"
# - Grok: "grok-4", "grok-3-mini"
# - DeepSeek: "deepseek-chat", "deepseek-reasoner"
model = "gpt-4o-mini"
# Warning threshold (0.75 = 75% of context window)
warning_threshold = 0.75
# Trigger compaction at this threshold (0.85 = 85% of context window)
compaction_threshold = 0.85
# Track individual component token usage (adds overhead)
detailed_tracking = false

[context.curation]
# Enable dynamic per-turn context curation (Phase 2 implementation)
enabled = true
# Maximum tokens to include per turn
max_tokens_per_turn = 100000
# Number of recent messages to always preserve
preserve_recent_messages = 5
# Maximum tool descriptions to include (phase-aware selection)
max_tool_descriptions = 10
# Include decision ledger summary in context
include_ledger = true
# Maximum ledger entries to include
ledger_max_entries = 12
# Include recent errors and resolutions
include_recent_errors = true
# Maximum recent errors to include
max_recent_errors = 3

[syntax_highlighting]
# Enable syntax highlighting for tool output
enabled = true

# Theme to use for syntax highlighting
# Can be an embedded theme name (e.g., "base16-ocean.dark", "base16-ocean.light")
# Or a path to a custom .tmTheme file
theme = "base16-ocean.dark"

# Enable theme caching for better performance
cache_themes = true

# Maximum file size for syntax highlighting (in MB)
max_file_size_mb = 10

# Languages to enable syntax highlighting for
enabled_languages = [
    "rust", "python", "javascript", "typescript", "go", "java",
    "cpp", "c", "php", "html", "css", "sql", "csharp", "bash"
]

# Performance settings
highlight_timeout_ms = 5000
